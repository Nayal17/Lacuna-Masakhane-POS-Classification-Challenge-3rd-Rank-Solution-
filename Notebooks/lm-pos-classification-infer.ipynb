{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:00:09.345164Z","iopub.status.busy":"2023-09-17T19:00:09.344316Z","iopub.status.idle":"2023-09-17T19:00:09.353528Z","shell.execute_reply":"2023-09-17T19:00:09.352362Z","shell.execute_reply.started":"2023-09-17T19:00:09.345116Z"},"trusted":true},"outputs":[],"source":["!pip install -q transformers\n","!pip install -q sentencepiece"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:00:09.387318Z","iopub.status.busy":"2023-09-17T19:00:09.386323Z","iopub.status.idle":"2023-09-17T19:00:25.168820Z","shell.execute_reply":"2023-09-17T19:00:25.167676Z","shell.execute_reply.started":"2023-09-17T19:00:09.387279Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]}],"source":["import os \n","import gc\n","import re\n","import math\n","import glob\n","import random\n","import warnings\n","import collections \n","import numpy as np\n","import pandas as pd\n","\n","\n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    AutoModel,\n","    get_cosine_schedule_with_warmup,\n",")\n","\n","from tqdm.notebook import tqdm\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:00:25.175400Z","iopub.status.busy":"2023-09-17T19:00:25.172946Z","iopub.status.idle":"2023-09-17T19:00:25.186604Z","shell.execute_reply":"2023-09-17T19:00:25.185583Z","shell.execute_reply.started":"2023-09-17T19:00:25.175368Z"},"trusted":true},"outputs":[],"source":["class cfg:\n","    seed = 2023\n","    batch_size = 64\n","    device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n","    \n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    \n","seed_everything(seed=cfg.seed)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:00:25.189516Z","iopub.status.busy":"2023-09-17T19:00:25.188821Z","iopub.status.idle":"2023-09-17T19:00:25.293957Z","shell.execute_reply":"2023-09-17T19:00:25.292657Z","shell.execute_reply.started":"2023-09-17T19:00:25.189480Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Word</th>\n","      <th>Language</th>\n","      <th>Pos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Id00qog2f11n_0</td>\n","      <td>Ne</td>\n","      <td>luo</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Id00qog2f11n_1</td>\n","      <td>otim</td>\n","      <td>luo</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Id00qog2f11n_2</td>\n","      <td>penj</td>\n","      <td>luo</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Id00qog2f11n_3</td>\n","      <td>e</td>\n","      <td>luo</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Id00qog2f11n_4</td>\n","      <td>kind</td>\n","      <td>luo</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Id  Word Language  Pos\n","0  Id00qog2f11n_0    Ne      luo  NaN\n","1  Id00qog2f11n_1  otim      luo  NaN\n","2  Id00qog2f11n_2  penj      luo  NaN\n","3  Id00qog2f11n_3     e      luo  NaN\n","4  Id00qog2f11n_4  kind      luo  NaN"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Pos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Id00qog2f11n_0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Id00qog2f11n_1</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Id00qog2f11n_2</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Id00qog2f11n_3</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Id00qog2f11n_4</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Id  Pos\n","0  Id00qog2f11n_0  NaN\n","1  Id00qog2f11n_1  NaN\n","2  Id00qog2f11n_2  NaN\n","3  Id00qog2f11n_3  NaN\n","4  Id00qog2f11n_4  NaN"]},"metadata":{},"output_type":"display_data"}],"source":["test_df = pd.read_csv(\"/content/Test.csv\")\n","sub_df = pd.read_csv(\"/content/SampleSubmission.csv\")\n","display(test_df.head())\n","display(sub_df.head())"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:00:25.297912Z","iopub.status.busy":"2023-09-17T19:00:25.297431Z","iopub.status.idle":"2023-09-17T19:00:25.313887Z","shell.execute_reply":"2023-09-17T19:00:25.312702Z","shell.execute_reply.started":"2023-09-17T19:00:25.297871Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total tags: 17\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Word</th>\n","      <th>Language</th>\n","      <th>Pos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Id00qog2f11n_0</td>\n","      <td>Ne</td>\n","      <td>luo</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Id00qog2f11n_1</td>\n","      <td>otim</td>\n","      <td>luo</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Id00qog2f11n_2</td>\n","      <td>penj</td>\n","      <td>luo</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Id00qog2f11n_3</td>\n","      <td>e</td>\n","      <td>luo</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Id00qog2f11n_4</td>\n","      <td>kind</td>\n","      <td>luo</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Id  Word Language  Pos\n","0  Id00qog2f11n_0    Ne      luo  NaN\n","1  Id00qog2f11n_1  otim      luo  NaN\n","2  Id00qog2f11n_2  penj      luo  NaN\n","3  Id00qog2f11n_3     e      luo  NaN\n","4  Id00qog2f11n_4  kind      luo  NaN"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["tags = ['ADJ',\n"," 'ADP',\n"," 'ADV',\n"," 'AUX',\n"," 'CCONJ',\n"," 'DET',\n"," 'INTJ',\n"," 'NOUN',\n"," 'NUM',\n"," 'PART',\n"," 'PRON',\n"," 'PROPN',\n"," 'PUNCT',\n"," 'SCONJ',\n"," 'SYM',\n"," 'VERB',\n"," 'X']\n","\n","\n","label_vocab = {'PAD': -100, 'ADJ': 0, 'ADP': 1, 'ADV': 2, 'AUX': 3, 'CCONJ': 4, 'DET': 5,\\\n","               'INTJ': 6, 'NOUN': 7, 'NUM': 8, 'PART': 9,\\\n","               'PRON': 10, 'PROPN': 11, 'PUNCT': 12, 'SCONJ': 13, 'SYM': 14, 'VERB': 15, 'X': 16}\n","\n","print(f\"Total tags: {len(tags)}\")\n","test_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:00:25.316489Z","iopub.status.busy":"2023-09-17T19:00:25.315512Z","iopub.status.idle":"2023-09-17T19:00:27.285889Z","shell.execute_reply":"2023-09-17T19:00:27.284883Z","shell.execute_reply.started":"2023-09-17T19:00:25.316459Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcaa6be84d6c43f6815a3bad02dbd5a7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/32045 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def get_samples(df):\n","    sentences = []\n","    current_sentence = []\n","\n","    # Process each row in the CSV data\n","    for index, row in tqdm(df.iterrows(), total=len(df)):\n","        word = row['Word']\n","            \n","        # removing soft hyphens\n","        word = word.replace('\\x8d', '') \n","        \n","        current_sentence.append(word)\n","\n","        # Check if the word ends with a full stop\n","        if word.strip() in ['.','?','!']:\n","            sentences.append(current_sentence)\n","            current_sentence = []\n","\n","    return sentences\n","\n","t_sentences = get_samples(test_df)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:00:27.288099Z","iopub.status.busy":"2023-09-17T19:00:27.287425Z","iopub.status.idle":"2023-09-17T19:00:27.296579Z","shell.execute_reply":"2023-09-17T19:00:27.295696Z","shell.execute_reply.started":"2023-09-17T19:00:27.288059Z"},"trusted":true},"outputs":[],"source":["def align_tokenizations(sentences, tokenizer):\n","    tokenized_sentences = []\n","    label_mask = [] # to pick up only the prediction of first token of the word\n","    for sentence in tqdm(sentences, total=len(sentences)):\n","        tok_sent = []\n","        lm_sent = []\n","        for word in sentence:\n","            word_tokens = tokenizer.tokenize(word)\n","            token_mask = [1] + [0]*(len(word_tokens)-1)\n","            tok_sent.extend(word_tokens)\n","            lm_sent.extend(token_mask)\n","        \n","        tokenized_sentences.append(tok_sent)\n","        label_mask.append(lm_sent)\n","\n","    return tokenized_sentences, label_mask"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:00:27.298870Z","iopub.status.busy":"2023-09-17T19:00:27.298002Z","iopub.status.idle":"2023-09-17T19:00:27.308997Z","shell.execute_reply":"2023-09-17T19:00:27.307982Z","shell.execute_reply.started":"2023-09-17T19:00:27.298822Z"},"trusted":true},"outputs":[],"source":["def convert_to_ids(sentences, lb_mask, tokenizer):\n","    sentences_ids = []\n","    label_mask = []\n","    for i, (sentence, mask) in tqdm(enumerate(zip(sentences, lb_mask)), total=len(sentences)):\n","        sentence_tensor = torch.tensor(tokenizer.convert_tokens_to_ids(['<s>'] + sentence + ['</s>'])).long()\n","        mask_tensor = torch.tensor([0] + mask + [0]).long()\n","        sentences_ids.insert(i, sentence_tensor)\n","        label_mask.insert(i, mask_tensor)\n","        \n","    return sentences_ids, label_mask"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:00:27.312401Z","iopub.status.busy":"2023-09-17T19:00:27.312077Z","iopub.status.idle":"2023-09-17T19:00:27.324479Z","shell.execute_reply":"2023-09-17T19:00:27.323118Z","shell.execute_reply.started":"2023-09-17T19:00:27.312375Z"},"trusted":true},"outputs":[],"source":["class PosTaggingDataset(Dataset):\n","    def __init__(self, sentences, label_mask):\n","        self.sentences = sentences\n","        self.label_mask = label_mask\n","\n","    def __getitem__(self, i):\n","        return self.sentences[i], self.label_mask[i]\n","\n","    def __len__(self):\n","        return len(self.sentences)\n","\n","def collate_fn(items):\n","    max_len = max(len(item[0]) for item in items) \n","    sentences = torch.ones((len(items), max_len)).long()\n","    attention_mask = torch.zeros((len(items), max_len)).long()\n","    label_mask = torch.zeros((len(items), max_len)).long()\n","    \n","    for i, (sentence, lb_mask)  in enumerate(items):\n","        att_mask = torch.tensor([1 for s in sentence]).long()\n","        lb_mask = torch.tensor([0 if s in [0,1,2] else m for s, m in zip(sentence, lb_mask)]) # 0: start, 1: pad, 2: end \n","        \n","        sentences[i][:len(sentence)] = sentence\n","        attention_mask[i][:len(att_mask)] = att_mask\n","        label_mask[i][:len(lb_mask)] = lb_mask\n","\n","    return {\n","        \"input_ids\": sentences , \n","        \"attention_mask\": attention_mask,\n","        \"label_mask\": label_mask\n","    }\n"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:00:27.326697Z","iopub.status.busy":"2023-09-17T19:00:27.326302Z","iopub.status.idle":"2023-09-17T19:00:27.339614Z","shell.execute_reply":"2023-09-17T19:00:27.338534Z","shell.execute_reply.started":"2023-09-17T19:00:27.326647Z"},"trusted":true},"outputs":[],"source":["class POSModel(nn.Module):\n","    def __init__(self, model_name, model_config, num_labels):\n","        super().__init__()\n","        \n","        self.base_model = AutoModel.from_pretrained(model_name, config=model_config)     \n","        self.linear = nn.Linear(self.base_model.config.hidden_size, num_labels)\n","            \n","    def forward(self, input_ids, attention_mask):\n","        word_emb, sentence_emb = self.base_model(input_ids, attention_mask, return_dict=False) \n","        logits = self.linear(word_emb)\n","        return logits\n","    "]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:00:27.344199Z","iopub.status.busy":"2023-09-17T19:00:27.343706Z","iopub.status.idle":"2023-09-17T19:00:27.356208Z","shell.execute_reply":"2023-09-17T19:00:27.355076Z","shell.execute_reply.started":"2023-09-17T19:00:27.344173Z"},"trusted":true},"outputs":[],"source":["def infer(model_name=\"\", model_path=\"\"):\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model_config = AutoConfig.from_pretrained(model_name)\n","    \n","    model = POSModel(model_name, model_config, len(label_vocab)-1)\n","    # model = nn.DataParallel(model, device_ids=[0, 1])\n","\n","    model.to(cfg.device)\n","    model.load_state_dict(torch.load(model_path, map_location=cfg.device))\n","    model.eval()\n","    \n","    tokenized_sentences, label_mask = align_tokenizations(t_sentences, tokenizer)\n","    sentences_ids, label_mask = convert_to_ids(tokenized_sentences, label_mask, tokenizer)\n","    loader = DataLoader(PosTaggingDataset(sentences_ids, label_mask), collate_fn=collate_fn, batch_size=cfg.batch_size, shuffle=False)\n","    \n","    preds = []\n","    for i, batch in tqdm(enumerate(loader), total=len(loader)):\n","        with torch.no_grad():\n","            logits = model(batch['input_ids'].to(cfg.device), batch['attention_mask'].to(cfg.device))\n","        \n","        logits = torch.max(logits.cpu(), 2)[1]\n","        label_mask = batch['label_mask']\n","\n","        \n","        label_mask = label_mask==1 # converting to bool\n","        logits = torch.masked_select(logits, label_mask)\n","        logits = logits.view(-1)\n","        preds.insert(i, logits.numpy())\n","        \n","    return preds"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:00:27.359486Z","iopub.status.busy":"2023-09-17T19:00:27.359136Z","iopub.status.idle":"2023-09-17T19:01:56.720661Z","shell.execute_reply":"2023-09-17T19:01:56.719522Z","shell.execute_reply.started":"2023-09-17T19:00:27.359453Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce77fbed901647bc94d84e1eab0deff7","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f3c65d6b76245a5a3a3a843c58eb279","version_major":2,"version_minor":0},"text/plain":["Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18f63ba3b34745189da47c9fbb6a662c","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db0b5493861546229559847e77f29d62","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76248a4d8d9b44ee88fb64142d1f5e37","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/714 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2d94318d30c4ff0897830de1c9e4364","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/3.27G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at Davlan/afro-xlmr-large-75L were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e99cb7ef566d4cde83cdbfba4d25b880","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1395 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"643fcb5df9e9406d81edf023b8c9486a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1395 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0a8c117af3a4c61b799c73fa8e1a073","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/22 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["all_preds = []\n","# for path in sorted(glob.glob(\"/kaggle/input/lm-pos-train-0-9-folds/*.pth\")):\n","path = \"/content/best_epoch.pth\"\n","model_preds = infer(model_name=\"Davlan/afro-xlmr-large-75L\", model_path=str(path))\n","model_preds = np.concatenate(model_preds)\n","all_preds.append(model_preds)\n","    \n","    "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:01:56.722573Z","iopub.status.busy":"2023-09-17T19:01:56.722090Z","iopub.status.idle":"2023-09-17T19:01:56.728801Z","shell.execute_reply":"2023-09-17T19:01:56.727579Z","shell.execute_reply.started":"2023-09-17T19:01:56.722536Z"},"trusted":true},"outputs":[],"source":["def pick_tags(x):\n","    counts = np.bincount(x)\n","    max_index = np.argmax(counts)\n","    return max_index"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:01:56.734559Z","iopub.status.busy":"2023-09-17T19:01:56.732429Z","iopub.status.idle":"2023-09-17T19:01:57.046659Z","shell.execute_reply":"2023-09-17T19:01:57.045517Z","shell.execute_reply.started":"2023-09-17T19:01:56.734518Z"},"trusted":true},"outputs":[],"source":["predictions = np.apply_along_axis(pick_tags, axis=0, arr=all_preds)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-09-17T19:01:57.048759Z","iopub.status.busy":"2023-09-17T19:01:57.048371Z","iopub.status.idle":"2023-09-17T19:01:57.178895Z","shell.execute_reply":"2023-09-17T19:01:57.177948Z","shell.execute_reply.started":"2023-09-17T19:01:57.048723Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Pos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Id00qog2f11n_0</td>\n","      <td>AUX</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Id00qog2f11n_1</td>\n","      <td>VERB</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Id00qog2f11n_2</td>\n","      <td>NOUN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Id00qog2f11n_3</td>\n","      <td>ADP</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Id00qog2f11n_4</td>\n","      <td>ADP</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Id00qog2f11n_5</td>\n","      <td>PROPN</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Id00qog2f11n_6</td>\n","      <td>NOUN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Id00qog2f11n_7</td>\n","      <td>NUM</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Id00qog2f11n_8</td>\n","      <td>CCONJ</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Id00qog2f11n_9</td>\n","      <td>PROPN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Id    Pos\n","0  Id00qog2f11n_0    AUX\n","1  Id00qog2f11n_1   VERB\n","2  Id00qog2f11n_2   NOUN\n","3  Id00qog2f11n_3    ADP\n","4  Id00qog2f11n_4    ADP\n","5  Id00qog2f11n_5  PROPN\n","6  Id00qog2f11n_6   NOUN\n","7  Id00qog2f11n_7    NUM\n","8  Id00qog2f11n_8  CCONJ\n","9  Id00qog2f11n_9  PROPN"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["label_to_pos = {value: key for key, value in label_vocab.items()}\n","sub_df['Pos'] = [label_to_pos[p] for p in predictions]\n","sub_df.to_csv(\"submission.csv\", index=False)\n","sub_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
